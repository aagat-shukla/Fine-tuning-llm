{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca7b133",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-20T10:20:57.179243Z",
     "iopub.status.busy": "2024-01-20T10:20:57.178568Z",
     "iopub.status.idle": "2024-01-20T10:20:57.891967Z",
     "shell.execute_reply": "2024-01-20T10:20:57.890878Z"
    },
    "papermill": {
     "duration": 0.730601,
     "end_time": "2024-01-20T10:20:57.894357",
     "exception": false,
     "start_time": "2024-01-20T10:20:57.163756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ea9312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:20:58.023571Z",
     "iopub.status.busy": "2024-01-20T10:20:58.023093Z",
     "iopub.status.idle": "2024-01-20T10:21:19.146843Z",
     "shell.execute_reply": "2024-01-20T10:21:19.145895Z"
    },
    "papermill": {
     "duration": 21.139912,
     "end_time": "2024-01-20T10:21:19.149213",
     "exception": false,
     "start_time": "2024-01-20T10:20:58.009301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "s3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc1d573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:21:19.177105Z",
     "iopub.status.busy": "2024-01-20T10:21:19.176788Z",
     "iopub.status.idle": "2024-01-20T10:21:19.181257Z",
     "shell.execute_reply": "2024-01-20T10:21:19.180432Z"
    },
    "papermill": {
     "duration": 0.020442,
     "end_time": "2024-01-20T10:21:19.183228",
     "exception": false,
     "start_time": "2024-01-20T10:21:19.162786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b30b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:37:06.372914Z",
     "iopub.status.busy": "2024-01-20T06:37:06.372043Z",
     "iopub.status.idle": "2024-01-20T06:37:06.386043Z",
     "shell.execute_reply": "2024-01-20T06:37:06.385137Z",
     "shell.execute_reply.started": "2024-01-20T06:37:06.372877Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f2257",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a name='2'></a>\n",
    "#### 2. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d5eab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:37:09.803154Z",
     "iopub.status.busy": "2024-01-20T06:37:09.802328Z",
     "iopub.status.idle": "2024-01-20T06:37:14.800161Z",
     "shell.execute_reply": "2024-01-20T06:37:14.799265Z",
     "shell.execute_reply.started": "2024-01-20T06:37:09.803122Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/neil-code/dialogsum-test\n",
    "huggingface_dataset_name = \"neil-code/dialogsum-test\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e343d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:37:22.699920Z",
     "iopub.status.busy": "2024-01-20T06:37:22.699549Z",
     "iopub.status.idle": "2024-01-20T06:37:22.706866Z",
     "shell.execute_reply": "2024-01-20T06:37:22.705981Z",
     "shell.execute_reply.started": "2024-01-20T06:37:22.699890Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd3b445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:37:33.444907Z",
     "iopub.status.busy": "2024-01-20T06:37:33.444536Z",
     "iopub.status.idle": "2024-01-20T06:37:33.451948Z",
     "shell.execute_reply": "2024-01-20T06:37:33.450994Z",
     "shell.execute_reply.started": "2024-01-20T06:37:33.444875Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80deea28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:37:36.663390Z",
     "iopub.status.busy": "2024-01-20T06:37:36.662683Z",
     "iopub.status.idle": "2024-01-20T06:37:55.595724Z",
     "shell.execute_reply": "2024-01-20T06:37:55.594708Z",
     "shell.execute_reply.started": "2024-01-20T06:37:36.663357Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name='microsoft/phi-2'\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc130317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:07.191825Z",
     "iopub.status.busy": "2024-01-20T06:38:07.190976Z",
     "iopub.status.idle": "2024-01-20T06:38:09.259675Z",
     "shell.execute_reply": "2024-01-20T06:38:09.258772Z",
     "shell.execute_reply.started": "2024-01-20T06:38:07.191780Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208640a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:11.288479Z",
     "iopub.status.busy": "2024-01-20T06:38:11.288093Z",
     "iopub.status.idle": "2024-01-20T06:38:16.216599Z",
     "shell.execute_reply": "2024-01-20T06:38:16.215607Z",
     "shell.execute_reply.started": "2024-01-20T06:38:11.288438Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34fc4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:17.767613Z",
     "iopub.status.busy": "2024-01-20T06:38:17.766951Z",
     "iopub.status.idle": "2024-01-20T06:38:17.977517Z",
     "shell.execute_reply": "2024-01-20T06:38:17.976678Z",
     "shell.execute_reply.started": "2024-01-20T06:38:17.767578Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "\n",
    "def gen(model,p, maxlen=100, sample=True):\n",
    "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
    "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n",
    "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769cd77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:23.249577Z",
     "iopub.status.busy": "2024-01-20T06:38:23.248327Z",
     "iopub.status.idle": "2024-01-20T06:38:29.685369Z",
     "shell.execute_reply": "2024-01-20T06:38:29.684466Z",
     "shell.execute_reply.started": "2024-01-20T06:38:23.249528Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model,formatted_prompt,100,)\n",
    "#print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a142f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:41.361729Z",
     "iopub.status.busy": "2024-01-20T06:38:41.361011Z",
     "iopub.status.idle": "2024-01-20T06:38:41.368868Z",
     "shell.execute_reply": "2024-01-20T06:38:41.367673Z",
     "shell.execute_reply.started": "2024-01-20T06:38:41.361689Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction','output')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n",
    "    RESPONSE_KEY = \"### Output:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86c7de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:42.367542Z",
     "iopub.status.busy": "2024-01-20T06:38:42.366797Z",
     "iopub.status.idle": "2024-01-20T06:38:42.373767Z",
     "shell.execute_reply": "2024-01-20T06:38:42.372862Z",
     "shell.execute_reply.started": "2024-01-20T06:38:42.367511Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6378120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:44.012070Z",
     "iopub.status.busy": "2024-01-20T06:38:44.011319Z",
     "iopub.status.idle": "2024-01-20T06:38:44.018676Z",
     "shell.execute_reply": "2024-01-20T06:38:44.017721Z",
     "shell.execute_reply.started": "2024-01-20T06:38:44.012035Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfd6f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:45.504915Z",
     "iopub.status.busy": "2024-01-20T06:38:45.504201Z",
     "iopub.status.idle": "2024-01-20T06:38:45.509895Z",
     "shell.execute_reply": "2024-01-20T06:38:45.508897Z",
     "shell.execute_reply.started": "2024-01-20T06:38:45.504881Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38b25c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:38:49.770369Z",
     "iopub.status.busy": "2024-01-20T06:38:49.770020Z",
     "iopub.status.idle": "2024-01-20T06:39:02.630942Z",
     "shell.execute_reply": "2024-01-20T06:39:02.630214Z",
     "shell.execute_reply.started": "2024-01-20T06:38:49.770342Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Pre-process dataset\n",
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n",
    "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851c068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:02.632587Z",
     "iopub.status.busy": "2024-01-20T06:39:02.632296Z",
     "iopub.status.idle": "2024-01-20T06:39:02.637741Z",
     "shell.execute_reply": "2024-01-20T06:39:02.636760Z",
     "shell.execute_reply.started": "2024-01-20T06:39:02.632561Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {train_dataset.shape}\")\n",
    "print(f\"Validation: {eval_dataset.shape}\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82efdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:20.294539Z",
     "iopub.status.busy": "2024-01-20T06:39:20.293676Z",
     "iopub.status.idle": "2024-01-20T06:39:20.303534Z",
     "shell.execute_reply": "2024-01-20T06:39:20.302434Z",
     "shell.execute_reply.started": "2024-01-20T06:39:20.294505Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003c0a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:24.093412Z",
     "iopub.status.busy": "2024-01-20T06:39:24.092536Z",
     "iopub.status.idle": "2024-01-20T06:39:24.100571Z",
     "shell.execute_reply": "2024-01-20T06:39:24.099579Z",
     "shell.execute_reply.started": "2024-01-20T06:39:24.093380Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c5789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:31.284063Z",
     "iopub.status.busy": "2024-01-20T06:39:31.283152Z",
     "iopub.status.idle": "2024-01-20T06:39:31.698890Z",
     "shell.execute_reply": "2024-01-20T06:39:31.697918Z",
     "shell.execute_reply.started": "2024-01-20T06:39:31.284029Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4bcb3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:35.271906Z",
     "iopub.status.busy": "2024-01-20T06:39:35.271014Z",
     "iopub.status.idle": "2024-01-20T06:39:35.283574Z",
     "shell.execute_reply": "2024-01-20T06:39:35.282621Z",
     "shell.execute_reply.started": "2024-01-20T06:39:35.271872Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc60a57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:41.791778Z",
     "iopub.status.busy": "2024-01-20T06:39:41.790943Z",
     "iopub.status.idle": "2024-01-20T06:39:41.807977Z",
     "shell.execute_reply": "2024-01-20T06:39:41.807104Z",
     "shell.execute_reply.started": "2024-01-20T06:39:41.791743Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See how the model looks different now, with the LoRA adapters added:\n",
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f33b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:49.182295Z",
     "iopub.status.busy": "2024-01-20T06:39:49.181607Z",
     "iopub.status.idle": "2024-01-20T06:39:49.194550Z",
     "shell.execute_reply": "2024-01-20T06:39:49.193629Z",
     "shell.execute_reply.started": "2024-01-20T06:39:49.182259Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = './peft-dialogue-summary-training/final-checkpoint'\n",
    "import transformers\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947ce49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:56.204084Z",
     "iopub.status.busy": "2024-01-20T06:39:56.203399Z",
     "iopub.status.idle": "2024-01-20T06:39:56.209885Z",
     "shell.execute_reply": "2024-01-20T06:39:56.208967Z",
     "shell.execute_reply.started": "2024-01-20T06:39:56.204052Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_training_args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2e79b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T06:39:57.949207Z",
     "iopub.status.busy": "2024-01-20T06:39:57.948300Z",
     "iopub.status.idle": "2024-01-20T10:11:40.582353Z",
     "shell.execute_reply": "2024-01-20T10:11:40.581400Z",
     "shell.execute_reply.started": "2024-01-20T06:39:57.949172Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad525d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:12:09.229339Z",
     "iopub.status.busy": "2024-01-20T10:12:09.228538Z",
     "iopub.status.idle": "2024-01-20T10:12:09.234735Z",
     "shell.execute_reply": "2024-01-20T10:12:09.233805Z",
     "shell.execute_reply.started": "2024-01-20T10:12:09.229305Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e07b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:12:15.717375Z",
     "iopub.status.busy": "2024-01-20T10:12:15.716634Z",
     "iopub.status.idle": "2024-01-20T10:12:16.006683Z",
     "shell.execute_reply": "2024-01-20T10:12:16.005744Z",
     "shell.execute_reply.started": "2024-01-20T10:12:15.717341Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Free memory for merging weights\n",
    "del original_model\n",
    "del peft_trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701c631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:12:19.432308Z",
     "iopub.status.busy": "2024-01-20T10:12:19.431935Z",
     "iopub.status.idle": "2024-01-20T10:12:19.437916Z",
     "shell.execute_reply": "2024-01-20T10:12:19.436995Z",
     "shell.execute_reply.started": "2024-01-20T10:12:19.432277Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b7c9ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:12:23.438323Z",
     "iopub.status.busy": "2024-01-20T10:12:23.437949Z",
     "iopub.status.idle": "2024-01-20T10:12:27.860662Z",
     "shell.execute_reply": "2024-01-20T10:12:27.859789Z",
     "shell.execute_reply.started": "2024-01-20T10:12:23.438291Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model_id = \"microsoft/phi-2\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a26de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:12:35.287195Z",
     "iopub.status.busy": "2024-01-20T10:12:35.286321Z",
     "iopub.status.idle": "2024-01-20T10:12:35.492183Z",
     "shell.execute_reply": "2024-01-20T10:12:35.491261Z",
     "shell.execute_reply.started": "2024-01-20T10:12:35.287161Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df66d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:12:39.215138Z",
     "iopub.status.busy": "2024-01-20T10:12:39.214472Z",
     "iopub.status.idle": "2024-01-20T10:12:39.745376Z",
     "shell.execute_reply": "2024-01-20T10:12:39.744360Z",
     "shell.execute_reply.started": "2024-01-20T10:12:39.215104Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ba4ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:13:40.909063Z",
     "iopub.status.busy": "2024-01-20T10:13:40.908658Z",
     "iopub.status.idle": "2024-01-20T10:13:49.707535Z",
     "shell.execute_reply": "2024-01-20T10:13:49.706576Z",
     "shell.execute_reply.started": "2024-01-20T10:13:40.909027Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
    "\n",
    "peft_model_res = gen(ft_model,prompt,100,)\n",
    "peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "#print(peft_model_output)\n",
    "prefix, success, result = peft_model_output.partition('#End')\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32415e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:13:54.919616Z",
     "iopub.status.busy": "2024-01-20T10:13:54.918729Z",
     "iopub.status.idle": "2024-01-20T10:13:58.903914Z",
     "shell.execute_reply": "2024-01-20T10:13:58.903111Z",
     "shell.execute_reply.started": "2024-01-20T10:13:54.919578Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3063381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:16:20.440910Z",
     "iopub.status.busy": "2024-01-20T10:16:20.440537Z",
     "iopub.status.idle": "2024-01-20T10:18:49.394779Z",
     "shell.execute_reply": "2024-01-20T10:18:49.393788Z",
     "shell.execute_reply.started": "2024-01-20T10:16:20.440879Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
    "    \n",
    "    original_model_res = gen(original_model,prompt,100,)\n",
    "    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n",
    "    \n",
    "    peft_model_res = gen(ft_model,prompt,100,)\n",
    "    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "    #print(peft_model_output)\n",
    "    peft_model_text_output, success, result = peft_model_output.partition('#End')\n",
    "    \n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e829f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:18:49.396915Z",
     "iopub.status.busy": "2024-01-20T10:18:49.396626Z",
     "iopub.status.idle": "2024-01-20T10:19:04.322742Z",
     "shell.execute_reply": "2024-01-20T10:19:04.321453Z",
     "shell.execute_reply.started": "2024-01-20T10:18:49.396890Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544be228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:19:30.797808Z",
     "iopub.status.busy": "2024-01-20T10:19:30.796882Z",
     "iopub.status.idle": "2024-01-20T10:19:32.043480Z",
     "shell.execute_reply": "2024-01-20T10:19:32.042466Z",
     "shell.execute_reply.started": "2024-01-20T10:19:30.797774Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca9279",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:19:35.851880Z",
     "iopub.status.busy": "2024-01-20T10:19:35.851035Z",
     "iopub.status.idle": "2024-01-20T10:19:35.857618Z",
     "shell.execute_reply": "2024-01-20T10:19:35.856633Z",
     "shell.execute_reply.started": "2024-01-20T10:19:35.851846Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 47.590297,
   "end_time": "2024-01-20T10:21:41.280535",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-20T10:20:53.690238",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
